---
title: "Educational Code-switched Language Pipeline with Encoder-Decoder Architecture"
excerpt: "Developed encoder-decoder architecture with transfer learning for Mandarin-English code-switched instructional language processing<br/><img src='/images/500x300.png'>"
collection: portfolio
---

**Duration:** September 2024 â€“ December 2024  
**Location:** Stanford, CA  
**Course Instructor:** Professor Diyi Yang

## Project Overview
Developed an innovative encoder-decoder architecture with transfer learning to generate Mandarin-English code-switched instructional language with a Code-Mixing Index of 25.28%, achieving 60-64% human-rated naturalness.

## Key Achievements
* **Advanced NLP Architecture**: Implemented encoder-decoder model with transfer learning capabilities
* **High Performance**: Achieved 60-64% human-rated naturalness in generated code-switched content
* **Bilingual Processing**: Successfully handled Mandarin-English code-switching patterns
* **Data Preparation**: Deployed BERT-based Name Entity Recognition and Pre-trained Neural Machine Translation

## Technical Skills Applied
* Deep Learning Architecture Design
* Transfer Learning
* BERT-based NLP
* Neural Machine Translation
* Bilingual Language Processing
* Code-Mixing Analysis

## Impact
This project contributes to the field of multilingual education technology by creating more natural and effective code-switched instructional materials for bilingual learners.
